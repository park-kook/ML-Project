{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tM3Huo-w35V"
   },
   "source": [
    "1) Updated the code to increase MAP@10 score by leveraging sentence transformers. Sentence transformers provide enhanced semantic similarity measurements, which improve the accuracy of the matching process.\n",
    "\n",
    "2) Additionally, leveraged the partial match count to provide a fairer assessment of performance. This helps to account for partial matches that are relevant but, may not be exact, giving a more nuanced evaluation of the model's effectiveness.\n",
    "\n",
    "3) Refactored the code into an Object-Oriented Programming (OOP) structure to make it more modular, maintainable, and ready for production deployment. This includes encapsulating functionality within classes and defining clear interfaces for interacting with different components of the system.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42591,
     "status": "ok",
     "timestamp": 1721221669822,
     "user": {
      "displayName": "BYOUNGKOOK PARK",
      "userId": "03886581541171404230"
     },
     "user_tz": 240
    },
    "id": "SjZq9VsgDQu-",
    "outputId": "393cea0e-fda1-48d4-e10d-3f1d095c1518"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
      "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.9.6)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.11.1)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
      "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl) (2.3.0+cu121)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from trl) (4.42.4)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl) (0.8.5)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (1.13.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->trl) (12.5.82)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->trl) (0.19.1)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.7.1)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (1.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (17.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.82)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "fatal: destination path 'WANDS' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install datasets trl peft bitsandbytes accelerate sentence_transformers\n",
    "!pip install --upgrade transformers accelerate pyarrow\n",
    "\n",
    "#clone the git repo that contains the data and additional information about the dataset\n",
    "!git clone https://github.com/wayfair/WANDS.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4461001,
     "status": "ok",
     "timestamp": 1721226130819,
     "user": {
      "displayName": "BYOUNGKOOK PARK",
      "userId": "03886581541171404230"
     },
     "user_tz": 240
    },
    "id": "VuFrT1H5dK6-",
    "outputId": "0a62dcc0-e6e7-4c8c-a7b6-b0fb1fa1a451"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean Average Precision @10: 0.4018\n",
      "\n",
      "Mean Average Precision @10 - relevance score (Exact: 1, Partial: 0.7, Irrelevent: 0): 0.6269\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, query_file, product_file, label_file):\n",
    "        self.query_df = pd.read_csv(query_file, sep='\\t')\n",
    "        self.product_df = pd.read_csv(product_file, sep='\\t')\n",
    "        self.label_df = pd.read_csv(label_file, sep='\\t')\n",
    "\n",
    "    def preprocess_product_data(self):\n",
    "        self.product_df['average_rating_'] = self.product_df['average_rating'].fillna(0)\n",
    "        self.product_df['product_description_'] = self.product_df['product_description'].fillna('')\n",
    "        self.product_df['product_class_'] = self.product_df['product_class'].fillna('')\n",
    "        self.product_df['category hierarchy_'] = self.product_df['category hierarchy'].fillna('')\n",
    "        self.product_df['normalized_average_ratings'] = (\n",
    "            self.product_df['average_rating_'] - self.product_df['average_rating_'].min()\n",
    "        ) / (self.product_df['average_rating_'].max() - self.product_df['average_rating_'].min())\n",
    "        self.product_df['combined_text'] = (\n",
    "            self.product_df['product_name'] + ' ' +\n",
    "            self.product_df['product_class_'] + ' ' +\n",
    "            self.product_df['category hierarchy_'] + ' ' +\n",
    "            self.product_df['product_description_'] + ' ' +\n",
    "            self.product_df['normalized_average_ratings'].astype(str)\n",
    "        )\n",
    "        logging.info(\"Product data preprocessing complete.\")\n",
    "\n",
    "class EmbeddingGenerator:\n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def generate_embeddings(self, texts):\n",
    "        try:\n",
    "            embeddings = self.model.encode(texts, convert_to_tensor=True)\n",
    "            logging.info(\"Embeddings generated successfully.\")\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error generating embeddings: {e}\")\n",
    "            return None\n",
    "\n",
    "class SimilarityCalculator:\n",
    "    @staticmethod\n",
    "    def calculate_cosine_similarity(query_embeddings, product_embeddings):\n",
    "        num_queries = len(query_embeddings)\n",
    "        num_products = len(product_embeddings)\n",
    "        similarity_matrix = torch.zeros((num_queries, num_products))\n",
    "        product_embeddings_tensor = product_embeddings.clone().detach()\n",
    "\n",
    "        for i in range(num_queries):\n",
    "            query_2d = query_embeddings[i].reshape(1, -1)\n",
    "            similarities = F.cosine_similarity(query_2d, product_embeddings_tensor)\n",
    "            similarity_matrix[i] = similarities\n",
    "\n",
    "        logging.info(\"Cosine similarity calculation complete.\")\n",
    "        return similarity_matrix\n",
    "\n",
    "class Evaluator:\n",
    "    def __init__(self, label_df):\n",
    "        self.label_df = label_df\n",
    "        self.grouped_label_df = label_df.groupby('query_id')\n",
    "        self.relevance_mapping = {\n",
    "            'Exact': 1.0,\n",
    "            'Partial': 0.7,\n",
    "            'Irrelevant': 0.0\n",
    "        }\n",
    "\n",
    "    def get_relevant_matches_for_query(self, query_id):\n",
    "        query_group = self.grouped_label_df.get_group(query_id)\n",
    "        relevant_products = query_group[query_group['label'].isin(['Partial', 'Exact'])]\n",
    "        return relevant_products['product_id'].values\n",
    "\n",
    "    def get_exact_matches_for_query(self, query_id):\n",
    "        query_group = self.grouped_label_df.get_group(query_id)\n",
    "        exact_matches = query_group.loc[query_group['label'] == 'Exact']['product_id'].values\n",
    "        return exact_matches\n",
    "\n",
    "    def get_relevant_labels_for_query(self, query_id):\n",
    "        query_group = self.grouped_label_df.get_group(query_id)\n",
    "        return {p_id: self.relevance_mapping[label] for p_id, label in zip(query_group['product_id'], query_group['label'])}\n",
    "\n",
    "    @staticmethod\n",
    "    def map_at_k(true_ids, predicted_ids, relevance_labels=None, k=10):\n",
    "        if not len(true_ids) or not len(predicted_ids):\n",
    "            return 0.0\n",
    "\n",
    "        score = 0.0\n",
    "        num_hits = 0.0\n",
    "        num_score_hits = 0.0\n",
    "        for i, p_id in enumerate(predicted_ids[:k]):\n",
    "            if p_id in true_ids and p_id not in predicted_ids[:i]:\n",
    "                if relevance_labels:\n",
    "                    relevance_score = relevance_labels.get(p_id, 0.0)\n",
    "                    if relevance_score > 0:\n",
    "                        num_hits += 1.0\n",
    "                        num_score_hits += relevance_score\n",
    "                        score += num_score_hits / (i + 1.0)\n",
    "                else:\n",
    "                    num_hits += 1.0\n",
    "                    score += num_hits / (i + 1.0)\n",
    "\n",
    "        if relevance_labels:\n",
    "            return score / min(len(true_ids), k)\n",
    "        return score / min(len(true_ids), k)\n",
    "\n",
    "def main():\n",
    "    logging.info(\"Starting the retrieval process...\")\n",
    "\n",
    "    # Data loading and preprocessing\n",
    "    data_loader = DataLoader(\"WANDS/dataset/query.csv\", \"WANDS/dataset/product.csv\", \"WANDS/dataset/label.csv\")\n",
    "    data_loader.preprocess_product_data()\n",
    "\n",
    "    # Embedding generation\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L12-v2'  # Change this to use different models\n",
    "    # model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    # model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "    # model_name = 'sentence-transformers/paraphrase-MiniLM-L6-v2'\n",
    "    # model_name = 'sentence-transformers/distilbert-base-nli-stsb-mean-tokens'\n",
    "\n",
    "    embedding_generator = EmbeddingGenerator(model_name)\n",
    "    query_embeddings = embedding_generator.generate_embeddings(data_loader.query_df['query'].tolist())\n",
    "    product_embeddings = embedding_generator.generate_embeddings(data_loader.product_df['combined_text'].tolist())\n",
    "\n",
    "    if query_embeddings is None or product_embeddings is None:\n",
    "        logging.error(\"Failed to generate embeddings. Exiting...\")\n",
    "        return\n",
    "\n",
    "    # Calculate cosine similarities\n",
    "    similarity_calculator = SimilarityCalculator()\n",
    "    cosine_similarities = similarity_calculator.calculate_cosine_similarity(query_embeddings, product_embeddings)\n",
    "\n",
    "    # Rank products and evaluate\n",
    "    top_product_ids_all = []\n",
    "    for i in range(len(cosine_similarities)):\n",
    "        top_product_indices = cosine_similarities[i].argsort(descending=True)[:10]\n",
    "        top_product_ids = data_loader.product_df.iloc[top_product_indices]['product_id'].tolist()\n",
    "        top_product_ids_all.append(top_product_ids)\n",
    "\n",
    "    data_loader.query_df['top_product_ids'] = top_product_ids_all\n",
    "\n",
    "    evaluator = Evaluator(data_loader.label_df)\n",
    "\n",
    "    # Calculate MAP@10 without graded relevance\n",
    "    data_loader.query_df['relevant_ids'] = data_loader.query_df['query_id'].apply(evaluator.get_exact_matches_for_query)\n",
    "    data_loader.query_df['map@k'] = data_loader.query_df.apply(\n",
    "        lambda x: Evaluator.map_at_k(\n",
    "            x['relevant_ids'],\n",
    "            x['top_product_ids']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    mean_average_k = data_loader.query_df['map@k'].mean()\n",
    "    logging.info(f'Mean Average Precision @10: {mean_average_k:.4f}')\n",
    "    print(f'\\nMean Average Precision @10: {mean_average_k:.4f}')\n",
    "\n",
    "    # Calculate MAP@10 with graded relevance\n",
    "    data_loader.query_df['relevant_ids'] = data_loader.query_df['query_id'].apply(evaluator.get_relevant_matches_for_query)\n",
    "    data_loader.query_df['relevance_labels'] = data_loader.query_df['query_id'].apply(evaluator.get_relevant_labels_for_query)\n",
    "    data_loader.query_df['map@k'] = data_loader.query_df.apply(\n",
    "        lambda x: Evaluator.map_at_k(\n",
    "            x['relevant_ids'],\n",
    "            x['top_product_ids'],\n",
    "            x['relevance_labels']\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    average_map_k = data_loader.query_df['map@k'].mean()\n",
    "    logging.info(f'Mean Average Precision @10 - map: {average_map_k:.4f}')\n",
    "    print(f'\\nMean Average Precision @10 - relevance score (Exact: 1, Partial: 0.7, Irrelevent: 0): {average_map_k:.4f}')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aq0SOKpQiHn5"
   },
   "source": [
    "### Appendix - Microsoft's Phi2 large language model.\n",
    "\n",
    "The following function is to generate embeddings using Phi-2 for product descriptions. This function iteratively processes the descriptions in smaller batches to avoid memory issues. However, the Microsoft's Phi2 model was not pursued any further due to significnatly lower MAP@10 performance metric. Please refer to the Word document attachment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XWj60gm6u5am"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    GenerationConfig,\n",
    "    AutoConfig\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from huggingface_hub import login\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "\n",
    "# Load datasets for search queries, products, and labels\n",
    "query_df = pd.read_csv(\"WANDS/dataset/query.csv\", sep='\\t')\n",
    "product_df = pd.read_csv(\"WANDS/dataset/product.csv\", sep='\\t')\n",
    "label_df = pd.read_csv(\"WANDS/dataset/label.csv\", sep='\\t')\n",
    "\n",
    "# Configuration for 4-bit quantization to reduce model size and memory usage\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "    )\n",
    "\n",
    "# Authenticate with Hugging Face\n",
    "huggingface_token = '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'\n",
    "login(token=huggingface_token, add_to_git_credential=True)\n",
    "\n",
    "# Load the model and tokenizer from Hugging Face\n",
    "model_name='microsoft/phi-2'\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "config.output_hidden_states = True\n",
    "device_map = {\"\": 0}\n",
    "original_model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                      device_map=device_map,\n",
    "                                                      quantization_config=bnb_config,\n",
    "                                                      trust_remote_code=True,\n",
    "                                                      use_auth_token=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True,padding_side=\"left\",add_eos_token=True,add_bos_token=True,use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Print the hidden size of the model\n",
    "config = original_model.config\n",
    "print(f\"Hidden size: {config.hidden_size}\")\n",
    "\n",
    "# Prepare the model for training with Low-Rank Adaptation (LoRA)\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "original_model = prepare_model_for_kbit_training(original_model)\n",
    "config = LoraConfig(\n",
    "    r=32, #Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\n",
    "        'q_proj',\n",
    "        'k_proj',\n",
    "        'v_proj',\n",
    "        'dense'\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "original_model.gradient_checkpointing_enable()\n",
    "peft_model = get_peft_model(original_model, config)\n",
    "\n",
    "\n",
    "# Function to generate embeddings for product descriptions.\n",
    "# This function iteratively processes the descriptions in smaller batches to avoid memory issues.\n",
    "def get_embeddings(texts, batch_size):\n",
    "    all_embeddings = []\n",
    "    num_batches = (len(texts) + batch_size - 1) // batch_size  # Calculate number of batches\n",
    "\n",
    "    for i in tqdm(range(num_batches)):  # Use tqdm for progress bar\n",
    "        print(i)\n",
    "        start = i * batch_size\n",
    "        end = (i + 1) * batch_size\n",
    "        batch_texts = texts[start:end]\n",
    "        if not all(isinstance(text, str) for text in batch_texts):\n",
    "            raise ValueError(\"Input is not valid. Should be a list/tuple of strings.\")\n",
    "\n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            # outputs = peft_model(**inputs, output_hidden_states=True)\n",
    "            outputs = original_model(**inputs, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states\n",
    "\n",
    "        last_hidden_state = hidden_states[-1]\n",
    "        batch_embeddings = last_hidden_state.mean(dim=1).cpu().numpy()  # Move to CPU to save GPU memory\n",
    "        # batch_embeddings = last_hidden_state[:, 0, :].cpu().numpy()  # Move to CPU to save GPU memory\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "\n",
    "    return np.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "# Prepare product descriptions and combine texts for embedding generation\n",
    "product_df['product_description2'] = product_df['product_description'].fillna('')\n",
    "product_df['combined_text'] = product_df['product_name'] + ' ' + product_df['product_description2']\n",
    "valid_texts = product_df['combined_text'].tolist()\n",
    "product_embeddings = get_embeddings(valid_texts, batch_size=20)\n",
    "\n",
    "# Generate query embeddings\n",
    "text = query_df['query'].tolist()\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "with torch.no_grad():\n",
    "    outputs = original_model(**inputs, output_hidden_states=True)\n",
    "    # outputs = original_model(**inputs, output_hidden_states=True, output_scores = True)\n",
    "    hidden_states = outputs.hidden_states\n",
    "\n",
    "last_hidden_state= hidden_states[-1]\n",
    "# qurey_token_embeddings = last_hidden_state[:,0,:]\n",
    "query_embeddings = last_hidden_state.mean(dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate cosine similarity between query embeddings and product embeddings\n",
    "def calculate_cosine_similarity(query_embeddings, product_embeddings):\n",
    "    num_queries = len(query_embeddings)\n",
    "    num_products = len(product_embeddings)\n",
    "\n",
    "    # Create a tensor to store the similarity results\n",
    "    similarity_matrix = torch.zeros((num_queries, num_products))\n",
    "    product_embeddings_tensor = torch.tensor(product_embeddings)\n",
    "\n",
    "    for i in range(num_queries):\n",
    "        # Calculate cosine similarity between the i-th query and all product embeddings\n",
    "        query_2d = query_embeddings[i].reshape(1, -1)\n",
    "        similarities = F.cosine_similarity(query_2d, product_embeddings_tensor)\n",
    "        # similarities = cosine_similarity(query_embeddings[i], product_embeddings)\n",
    "        similarity_matrix[i] = similarities\n",
    "\n",
    "    return similarity_matrix\n",
    "\n",
    "# Calculate cosine similarities\n",
    "cosine_similarities = calculate_cosine_similarity(query_embeddings, product_embeddings)\n",
    "\n",
    "# Retrieve top 10 product IDs for each query based on cosine similarity\n",
    "top_product_ids_all = []\n",
    "for i in range(len(cosine_similarities)):\n",
    "    top_product_indices = cosine_similarities[i].argsort(descending=True)[:10]\n",
    "    top_product_ids = product_df.iloc[top_product_indices]['product_id'].tolist()\n",
    "    top_product_ids_all.append(top_product_ids)\n",
    "\n",
    "# Assign the list of lists to the DataFrame column\n",
    "query_df['top_product_ids'] = top_product_ids_all\n",
    "\n",
    "\n",
    "# Function to retrieve exact match product IDs for a query_id\n",
    "def get_exact_matches_for_query(query_id):\n",
    "    query_group = grouped_label_df.get_group(query_id)\n",
    "    exact_matches = query_group.loc[query_group['label'] == 'Exact']['product_id'].values\n",
    "    return exact_matches\n",
    "\n",
    "# Add the list of exact match product_IDs from labels_df\n",
    "query_df['relevant_ids'] = query_df['query_id'].apply(get_exact_matches_for_query)\n",
    "\n",
    "\n",
    "# Function to calculate Mean Average Precision at K (MAP@K)\n",
    "def map_at_k(true_ids, predicted_ids, k=10):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Average Precision at K (MAP@K).\n",
    "\n",
    "    Parameters:\n",
    "    true_ids (list): List of relevant product IDs.\n",
    "    predicted_ids (list): List of predicted product IDs.\n",
    "    k (int): Number of top elements to consider.\n",
    "             NOTE: IF you wish to change top k, please provide a justification for choosing the new value\n",
    "\n",
    "    Returns:\n",
    "    float: MAP@K score.\n",
    "    \"\"\"\n",
    "    if not len(true_ids) or not len(predicted_ids):\n",
    "        return 0.0\n",
    "\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "\n",
    "    for i, p_id in enumerate(predicted_ids[:k]):\n",
    "        if p_id in true_ids and p_id not in predicted_ids[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "\n",
    "    return score / min(len(true_ids), k)\n",
    "    # return score / max(num_hits, 0.00000001)\n",
    "\n",
    "# Assign the MAP@K score to the DataFrame\n",
    "query_df['map@k'] = query_df.apply(lambda x: map_at_k(x['relevant_ids'], x['top_product_ids'], k=10), axis=1)\n",
    "\n",
    "# Calculate the MAP across the entire query set\n",
    "mean_average_k = query_df.loc[:, 'map@k'].mean()\n",
    "print(f'\\nMean Average Precision @10: {mean_average_k:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMUl8yankfhEaY58CvoEj2+",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1vv6VWuAj2DahzstvYZsiduicwIg4I7tr",
     "timestamp": 1721179647138
    },
    {
     "file_id": "1BeWbiQjEBJMRKxNTwl14997mr9c36rIy",
     "timestamp": 1721144836673
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
